{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c423f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import io\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39dc0c",
   "metadata": {},
   "source": [
    "## OpenAI Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674d843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e27ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_68acb4af008c81919b4924472e209446\n"
     ]
    }
   ],
   "source": [
    "vector_store = client.vector_stores.create(\n",
    "    name=\"hot_links\"\n",
    ")\n",
    "print(vector_store.id)\n",
    "\n",
    "# Create the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca252d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_file_object(name,content):\n",
    "    return (name+'.txt', io.BytesIO(content.encode('utf-8')), 'text/markdown')\n",
    "# Important to encode as bytes and add '.txt' to filename to trick API that this is a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_file_to_cloud(name, content):\n",
    "    result = client.files.create(file = make_file_object(name, content), purpose = 'assistants') \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStoreFile(id='file-RJNaMAB5ipLTiv696Tai1e', created_at=1756149658, last_error=None, object='vector_store.file', status='in_progress', usage_bytes=0, vector_store_id='vs_68acb4af008c81919b4924472e209446', attributes={'a2': 'test'}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))\n"
     ]
    }
   ],
   "source": [
    "def add_file_to_db(file_id, attributes = {}):\n",
    "    result = client.vector_stores.files.create(\n",
    "        vector_store_id=vector_store.id,\n",
    "        file_id=file_id,\n",
    "        attributes=attributes\n",
    "    )\n",
    "    return result\n",
    "# Then add it to the vector store with attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88b7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_68acb79ea0a481968e1f2be3956ea55107af2b659c522373', created_at=1756149662.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-2025-04-14', object='response', output=[ResponseFileSearchToolCall(id='fs_68acb79ff0d48196a30ae2e48f26285b07af2b659c522373', queries=['What is wine made from?'], status='completed', type='file_search_call', results=None), ResponseOutputMessage(id='msg_68acb7a14c348196bcc4a429678b855307af2b659c522373', content=[ResponseOutputText(annotations=[AnnotationFileCitation(file_id='file-RJNaMAB5ipLTiv696Tai1e', filename='dummy_4.txt.txt', index=344, type='file_citation')], text='According to the search results in your file, wine is made from plums. However, traditionally, wine is most commonly made from fermented grapes, although it can be made from various fruits, including plums, apples, and others. In the snippet, there is a suggestion that wine is made from plums, perhaps humorously or as an example of fruit wine. \\n\\nIf you need clarification about a specific type of wine or a general answer, let me know!', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FileSearchTool(type='file_search', vector_store_ids=['vs_68acb4af008c81919b4924472e209446'], filters=ComparisonFilter(key='a2', type='eq', value='test'), max_num_results=20, ranking_options=RankingOptions(ranker='auto', score_threshold=0.0))], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=1948, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=126, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=2074), user=None, store=True)\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=\"What is wine made from?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id],\n",
    "        \"filters\": {\n",
    "            \"type\": \"eq\",\n",
    "            \"key\": \"a2\",\n",
    "            \"value\": \"test\"\n",
    "        }\n",
    "    }]\n",
    ")\n",
    "print(response)\n",
    "# Then query the store with filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStore(id='vs_68acb4af008c81919b4924472e209446', created_at=1756148911, file_counts=FileCounts(cancelled=0, completed=5, failed=0, in_progress=0, total=5), last_active_at=1756149657, metadata={}, name='hot_links', object='vector_store', status='completed', usage_bytes=5246, expires_after=None, expires_at=None, description=None)\n",
      "VectorStore(id='vs_68acae441e188191a605f82b03a98305', created_at=1756147268, file_counts=FileCounts(cancelled=0, completed=3, failed=0, in_progress=0, total=3), last_active_at=1756148425, metadata={}, name='hot_links', object='vector_store', status='completed', usage_bytes=3131, expires_after=None, expires_at=None, description=None)\n",
      "VectorStore(id='vs_68ac553757d88191873c6900f9c90c0c', created_at=1756124471, file_counts=FileCounts(cancelled=0, completed=994, failed=6, in_progress=0, total=1000), last_active_at=1756146900, metadata={}, name='hot_links_final2', object='vector_store', status='completed', usage_bytes=6545010, expires_after=None, expires_at=None, description=None)\n",
      "VectorStore(id='vs_68ac54fb91b0819180b65563cb88062e', created_at=1756124411, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1756124411, metadata={}, name='hot_links_final2', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None, description=None)\n",
      "VectorStore(id='vs_68ac547571e881918c5b003281473561', created_at=1756124277, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1756124277, metadata={}, name='hot_links_final', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None, description=None)\n",
      "VectorStore(id='vs_68ac4cff5f1c81919b3bfda45ea803bb', created_at=1756122367, file_counts=FileCounts(cancelled=0, completed=14, failed=0, in_progress=0, total=14), last_active_at=1756124056, metadata={}, name='hot_links', object='vector_store', status='completed', usage_bytes=84270, expires_after=None, expires_at=None, description=None)\n",
      "VectorStore(id='vs_68ab6e552c348191b066ae9baa26b73f', created_at=1756065365, file_counts=FileCounts(cancelled=0, completed=3, failed=0, in_progress=0, total=3), last_active_at=1756066749, metadata={}, name='hot_links', object='vector_store', status='completed', usage_bytes=3140, expires_after=None, expires_at=None, description=None)\n",
      "VectorStore(id='vs_68ab680fb1908191b4c9cf36e59ab145', created_at=1756063759, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1756063787, metadata={}, name='hot_links', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None, description=None)\n",
      "VectorStore(id='vs_68ab595999388191ad48c47f6d09bae2', created_at=1756059993, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1756060465, metadata={}, name='hot_links', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None, description=None)\n"
     ]
    }
   ],
   "source": [
    "def get_all_vs():\n",
    "    for f in client.vector_stores.list():\n",
    "        yield f\n",
    "# Get all vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba1b241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1217it [10:56,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm.tqdm(client.files.list()):\n",
    "    #print(f.id)\n",
    "\n",
    "    res = client.files.delete(f.id)\n",
    "    \n",
    "    if not res.deleted:\n",
    "        tqdm.tqdm.write(f\"Failed to delete file {f.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7beb0383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(100)):\n",
    "    result = client.files.create(file = make_file_object('dummy_{:d}.txt'.format(i), \"Wine is made from plums... right?\"), purpose='assistants')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e381143",
   "metadata": {},
   "source": [
    "Based on [OpenAI documentation](https://platform.openai.com/docs/guides/tools-file-search#how-to-use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4edb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = io.BytesIO(b\"some initial text data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1165cc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'some initial text data'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0c5d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types import FileObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9f12ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "FileObject(\n",
      "    *,\n",
      "    id: str,\n",
      "    bytes: int,\n",
      "    created_at: int,\n",
      "    filename: str,\n",
      "    object: Literal[\u001b[33m'file'\u001b[39m],\n",
      "    purpose: Literal[\u001b[33m'assistants'\u001b[39m, \u001b[33m'assistants_output'\u001b[39m, \u001b[33m'batch'\u001b[39m, \u001b[33m'batch_output'\u001b[39m, \u001b[33m'fine-tune'\u001b[39m, \u001b[33m'fine-tune-results'\u001b[39m, \u001b[33m'vision'\u001b[39m, \u001b[33m'user_data'\u001b[39m],\n",
      "    status: Literal[\u001b[33m'uploaded'\u001b[39m, \u001b[33m'processed'\u001b[39m, \u001b[33m'error'\u001b[39m],\n",
      "    expires_at: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    status_details: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    **extra_data: Any,\n",
      ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "!!! abstract \"Usage Documentation\"\n",
      "    [Models](../concepts/models.md)\n",
      "\n",
      "A base class for creating Pydantic models.\n",
      "\n",
      "Attributes:\n",
      "    __class_vars__: The names of the class variables defined on the model.\n",
      "    __private_attributes__: Metadata about the private attributes of the model.\n",
      "    __signature__: The synthesized `__init__` [`Signature`][inspect.Signature] of the model.\n",
      "\n",
      "    __pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n",
      "    __pydantic_core_schema__: The core schema of the model.\n",
      "    __pydantic_custom_init__: Whether the model has a custom `__init__` function.\n",
      "    __pydantic_decorators__: Metadata containing the decorators defined on the model.\n",
      "        This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n",
      "    __pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n",
      "        __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n",
      "    __pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n",
      "    __pydantic_post_init__: The name of the post-init method for the model, if defined.\n",
      "    __pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n",
      "    __pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n",
      "    __pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n",
      "\n",
      "    __pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "    __pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "\n",
      "    __pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n",
      "        is set to `'allow'`.\n",
      "    __pydantic_fields_set__: The names of fields explicitly set during instantiation.\n",
      "    __pydantic_private__: Values of private attributes set on the model instance.\n",
      "\u001b[31mInit docstring:\u001b[39m\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "validated to form a valid model.\n",
      "\n",
      "`self` is explicitly positional-only to allow `self` as a field name.\n",
      "\u001b[31mFile:\u001b[39m           /workspaces/hot-links/.venv/lib/python3.12/site-packages/openai/types/file_object.py\n",
      "\u001b[31mType:\u001b[39m           ModelMetaclass\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "?FileObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08feee0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BytesIO at 0x7b75d6f30220>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io.BytesIO('test'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92abe618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dummy_3.txt.txt', <_io.BytesIO at 0x76e349562ca0>, 'text/markdown')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_file_object('dummy_3.txt', \"Wine is made from grapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f4523ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-RJNaMAB5ipLTiv696Tai1e'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca5b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_file_object(file_object):\n",
    "    client.vector_stores.files.upload_and_poll(        # Upload file\n",
    "    vector_store_id=vector_store.id,\n",
    "    file=file_object   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d278048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreFile(id='file-Vqdvu9xspCKE49JZ6WoYFr', created_at=1756148927, last_error=None, object='vector_store.file', status='completed', usage_bytes=1048, vector_store_id='vs_68acb4af008c81919b4924472e209446', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.vector_stores.files.upload_and_poll(        # Upload file\n",
    "    vector_store_id=vector_store.id,\n",
    "    file=make_file_object('dummy_3.txt', \"Wine is made from grapes\")   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f9285a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-Vqdvu9xspCKE49JZ6WoYFr', bytes=24, created_at=1756148925, filename='dummy_3.txt.txt', object='file', purpose='assistants', status='processed', expires_at=None, status_details=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.files.retrieve(\"file-Vqdvu9xspCKE49JZ6WoYFr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faffeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=\"How is wine made?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246f8e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[VectorStoreSearchResponse](data=[VectorStoreSearchResponse(attributes={}, content=[Content(text='Wine is made from grapes', type='text')], file_id='file-Vqdvu9xspCKE49JZ6WoYFr', filename='dummy_3.txt.txt', score=0.5208766983986627)], object='vector_store.search_results.page', search_query=['How is wine made?'], has_more=False, next_page=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb4b5d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m make_file_object(name, content)\n",
      "\u001b[31mDocstring:\u001b[39m <no docstring>\n",
      "\u001b[31mSource:\u001b[39m   \n",
      "\u001b[38;5;28;01mdef\u001b[39;00m make_file_object(name,content):\n",
      "    \u001b[38;5;28;01mreturn\u001b[39;00m (name, io.BytesIO(content.encode(\u001b[33m'utf-8'\u001b[39m)), \u001b[33m'text/plain'\u001b[39m)\n",
      "\u001b[31mFile:\u001b[39m      /tmp/ipykernel_709/3135118408.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "??make_file_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dec3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_file_object(make_file_object('xxx','lorem ipsum'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae758299",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'File type not supported', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43madd_file_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_file_object\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxxx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbodyContent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36madd_file_object\u001b[39m\u001b[34m(file_object)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_file_object\u001b[39m(file_object):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_stores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_and_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Upload file\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvector_store_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_object\u001b[49m\u001b[43m   \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/hot-links/.venv/lib/python3.12/site-packages/openai/resources/vector_stores/files.py:389\u001b[39m, in \u001b[36mFiles.upload_and_poll\u001b[39m\u001b[34m(self, vector_store_id, file, attributes, poll_interval_ms, chunking_strategy)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add a file to a vector store and poll until processing is complete.\"\"\"\u001b[39;00m\n\u001b[32m    388\u001b[39m file_obj = \u001b[38;5;28mself\u001b[39m._client.files.create(file=file, purpose=\u001b[33m\"\u001b[39m\u001b[33massistants\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_and_poll\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvector_store_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_store_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunking_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunking_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpoll_interval_ms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoll_interval_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/hot-links/.venv/lib/python3.12/site-packages/openai/resources/vector_stores/files.py:312\u001b[39m, in \u001b[36mFiles.create_and_poll\u001b[39m\u001b[34m(self, file_id, vector_store_id, attributes, poll_interval_ms, chunking_strategy)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_and_poll\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    304\u001b[39m     file_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     chunking_strategy: FileChunkingStrategyParam | NotGiven = NOT_GIVEN,\n\u001b[32m    310\u001b[39m ) -> VectorStoreFile:\n\u001b[32m    311\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Attach a file to the given vector store and wait for it to be processed.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvector_store_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_store_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunking_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunking_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattributes\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.poll(\n\u001b[32m    317\u001b[39m         file_id,\n\u001b[32m    318\u001b[39m         vector_store_id=vector_store_id,\n\u001b[32m    319\u001b[39m         poll_interval_ms=poll_interval_ms,\n\u001b[32m    320\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/hot-links/.venv/lib/python3.12/site-packages/openai/resources/vector_stores/files.py:92\u001b[39m, in \u001b[36mFiles.create\u001b[39m\u001b[34m(self, vector_store_id, file_id, attributes, chunking_strategy, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a non-empty value for `vector_store_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvector_store_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mOpenAI-Beta\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistants=v2\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/vector_stores/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvector_store_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/files\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattributes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchunking_strategy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunking_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFileCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVectorStoreFile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/hot-links/.venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/hot-links/.venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'File type not supported', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "add_file_object(make_file_object('xxx',df.head()['bodyContent'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7dc0fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).apply(lambda row: make_file_object(row['webUrl'], row['bodyContent']), axis=1).apply(add_file_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06835eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
